{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExPred\n",
    "This notebook can be used generate parts of version 1, as well as version 2 of the prototypes for presentations of the attribution technique ExPred\n",
    ##### "Required Input: Path to fever jsonl data set. (e.g. test.jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import torch\n",
    "from src.expred import (seeding, ExpredInput,\n",
    "                        BertTokenizerWithSpans, ExpredConfig, Expred)\n",
    "from src.expred.models import (prepare_for_cl, train_evidence_classifier,\n",
    "                               train_mtl_token_identifier)\n",
    "from transformers import BertTokenizer\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform the input to the way the expred accepts\n",
    "def formatData(path_to_file, path_to_fever_docs):\n",
    "    data = []\n",
    "    with open(path_to_file, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        label = result.get('classification')\n",
    "        query = result.get('query').split()\n",
    "        for docs_id in result.get('docids'):\n",
    "            with open(f'{path_to_fever_docs}/{docs_id}', 'r') as json_file_2:\n",
    "                documents = list(json_file_2)\n",
    "                contexts = []\n",
    "                for doc in documents:\n",
    "                    contexts.append(doc.split())\n",
    "            data.append([label,query,contexts])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function with code from expred showcase\n",
    "def getExPredResult(data): \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    expred_config = ExpredConfig(\n",
    "        pretrained_dataset_name='fever',\n",
    "        base_dataset_name='fever',\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        load_from_pretrained=True)\n",
    "\n",
    "    # seeding\n",
    "    seeding(1234)\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizerWithSpans.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # create the model\n",
    "    expred = Expred.from_pretrained(expred_config)\n",
    "    expred.eval()\n",
    "\n",
    "    for data_point in data:\n",
    "        # print query and data to paste into figma template\n",
    "        print('Query: ', ' '.join(data_point[1]))\n",
    "        # flatten whole data \n",
    "        print('Whole data: ', ' '.join([item for sublist in data_point[2] for item in sublist]))\n",
    "        expred_input = ExpredInput(\n",
    "            queries = [data_point[1]],\n",
    "            docs = [data_point[2]],\n",
    "            labels = [data_point[0]],\n",
    "            config = expred_config,\n",
    "            ann_ids = ['spontan_1'],\n",
    "            span_tokenizer = tokenizer)\n",
    "        # don't forget to preprocess\n",
    "        expred_input.preprocess()\n",
    "\n",
    "        # the output is in the form of a dict:\n",
    "        expred_output = expred(expred_input)\n",
    "\n",
    "        # functions from ExPred\n",
    "        expred_preds_output = expred_input.get_decoded_exp_preds(expred_output)[0]\n",
    "        # optionally remove dots\n",
    "        # expred_preds_output = [i for i in expred_preds_output if i!=('.')]\n",
    "        print('Influencial tokens: ', ' '.join(expred_preds_output))\n",
    "        print('Classification: ',expred_input.get_decoded_cls_preds(expred_output)[0])"
   ]
  }
  }
